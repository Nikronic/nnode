{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "      TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "    });MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the code in the `nnpde1.py` module. The module allows the user to solve a 1st-order partial differential equation (PDE) with Dirichlet boundary conditions specified on all boundaries, using a single-hidden layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is based on the paper [\"Artificial Neural Networks for Solving Ordinary and Partial Differential Equations\", by Lagaris et al, *IEEE Transactions on Neural Networks, Volume 9, No. 5*, September 1998](http://ieeexplore.ieee.org/document/712178/). Note that the notation used in this notebook and the associated Python code code differs slightly from that used in the Lagaris paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an $m$-dimensional space containing vectors $\\vec x = (x_1,x_2,...,x_m)$. Any 1st-order PDE for the scalar function $\\psi(\\vec x)$ can be written in the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(\\vec x,\\psi,\\vec \\nabla \\psi) = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to find a suitable solution to the PDE using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is trained using a set of $n$ training points $\\vec x_i$ ($1 \\leq i \\leq n$). This work assumes that the vectors $\\vec x_i$ have been scaled so that each component $x_{ij} \\in [0,1]$, $1 \\leq j \\leq m$. The training points need not be evenly-spaced. Note that only the independent variable vectors $\\vec x_i$  of the training points are needed - the estimated value of the solution at the training points is obtained using a trial solution. For a 1st-order PDE with all Dirichlet boundary conditions, the trial solution has the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\psi_t(\\vec x_i,\\vec p) = A(\\vec x_i) + P(\\vec x_i) N(\\vec x_i,\\vec p)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\psi_{ti} = A_i + P_i N_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\psi_{ti}$ is the value of the trial solution at the current training point $\\vec x_i$, $A_i$ is a function which yields the boundary conditions on each boundary, $P_i$ is a function which forces the product $P_i N_i$ to vanish at the boundaries, and $N_i$ is the floating-point output from an unspecified neural network with network parameters $\\vec p$. Note that this trial solution satisfies the boundary conditions by construction - at any boundary, the second term vanishes, leaving $\\psi_t(\\vec x_i)=A(\\vec x_i)$, which is just the specified boundary condition on that boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in a set of *epochs*. A training epoch consists of presenting the neural network with each of the $n$ training points $\\vec x_i$, one at a time. For each input point $\\vec x_i$, the network output $N_i$ is computed. Once all $n$ points have been presented, the epoch is complete, and the error function $E$ is computed. The problem definition provides an analytical form for the error function, as the sum of squared errors (SSE) for each of the training points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E = \\sum_{i=1}^{n} \\left( G(\\vec x_i,\\psi_{ti}, \\vec \\nabla \\psi_{ti}) \\right)^2 =\n",
    "\\sum_{i=1}^{n} G_i^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this error function is computed, the parameters in the neural network are adjusted to reduce the error. Eventually, a minimum of $E$ is attained, and the resulting final form of $\\psi_t(x)$ is used as the solution to the original PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the network output $N_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network used in this work has a simple structure. One input node for each component of $\\vec x_i$ is used (for a total of $m$ input nodes) to provide the training data. Each input node is fully-connected to each of a set of $H$ hidden nodes, each using a sigmoid transfer function. Each hidden node is connected to the single output node, which uses a linear transfer function with a weight for the signal from each hidden node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each step of a training epoch, the input to the network is just the training point $\\vec x_i$. Each input node $j$ receives one component $x_{ij}$, and emits that value as output. These outputs are sent to each of the $H$ hidden nodes. At each hidden node $k$, the input values $x_{ij}$ are combined and scaled by the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z_{ik} = \\sum_{j=1}^m w_{jk} x_{ij} + u_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where  $w_{jk}$ is the weight for input $x_{ij}$ at hidden node $k$, and $u_k$ is the bias at hidden node $k$. This combined value is then used as the input to a sigmoidal transfer function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sigma_{ik} = \\sigma(z_{ik})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac {1}{1+e^{-z}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of this transfer function and its first several derivatives is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1 / (1 + exp(-z))\n",
    "\n",
    "def dsigma_dz(z):\n",
    "    return exp(-z) / (1 + exp(-z))**2\n",
    "\n",
    "def d2sigma_dz2(z):\n",
    "    return (\n",
    "        2 * exp(-2 * z) / (1 + exp(-z))**3 - exp(-z) / (1 + exp(-z))**2\n",
    "    )\n",
    "\n",
    "def d3sigma_dz3(z):\n",
    "    return (\n",
    "        6 * exp(-3 * z) / (1 + exp(-z))**4\n",
    "        - 6 * exp(-2 * z) / (1 + exp(-z))**3\n",
    "        + exp(-z) / (1 + exp(-z))**2\n",
    "    )\n",
    "\n",
    "z = np.arange(-5, 5, 0.01)\n",
    "n = len(z)\n",
    "s = np.zeros(n)\n",
    "ds_dz = np.zeros(n)\n",
    "d2s_dz2 = np.zeros(n)\n",
    "d3s_dz3 = np.zeros(n)\n",
    "for i in range(n):\n",
    "    s[i] = sigma(z[i])\n",
    "    ds_dz[i] = dsigma_dz(z[i])\n",
    "    d2s_dz2[i] = d2sigma_dz2(z[i])\n",
    "    d3s_dz3[i] = d3sigma_dz3(z[i])\n",
    "plt.plot(z,s,label = \"$\\sigma(z)$\")\n",
    "plt.plot(z,ds_dz,label = \"$d\\sigma/dz$\")\n",
    "plt.plot(z,d2s_dz2, label = \"$d^2\\sigma/dz^2$\")\n",
    "plt.plot(z,d3s_dz3, label = \"$d^3\\sigma/dz^3$\");\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"$\\sigma(z)$ or derivative\")\n",
    "plt.title(\"Figure 1: The $\\sigma$-function and its first three derivatives\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the $\\sigma_{ik}$ are computed, they are all passed to the single output node, where they are processed by a linear transfer function to create the network output for the current input point $\\vec x_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "N_i = \\sum_{k=1}^{H}v_k\\sigma_{ik}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $v_k$ is the weight applied to the output from hidden node $k$ at the output node. Once $N_i$ has been computed, the trial function $\\psi_{ti}$ is computed. Next, we need the gradient of the trial function $\\nabla \\psi_{ti}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\vec \\nabla \\psi_{ti} = \\vec \\nabla (A_i + P_i N_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each component $j$ of this gradient may be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial \\psi_{ti}}{\\partial x_{ij}} =\n",
    "\\frac {\\partial A_i}{\\partial x_{ij}} + \\frac {\\partial}{\\partial x_{ij}} \\left( P_i N_i \\right) =\n",
    "\\frac {\\partial A_i}{\\partial x_{ij}} + P_i \\frac {\\partial N_i}{\\partial x_{ij}} + \\frac {\\partial P_i}{\\partial x_{ij}} N_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several other intermediate derivatives are needed to compute the values need in the computation of the error function $E$. The derivatives of $A_i$ and $P_i$ with respect to $x_{ij}$ are computed from their known analytical forms, which may vary based on the problem under investigation. The values of the network derivatives $\\frac {\\partial N_i}{\\partial x_{ij}}$ are computed analytically using the known form of the network and its weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial N_i}{\\partial x_{ij}} =\n",
    "\\frac {\\partial}{\\partial x_{ij}} \\sum_{k=1}^{H} v_k \\sigma_{ik} =\n",
    "\\sum_{k=1}^{H} v_k \\frac {\\partial \\sigma_{ik}}{\\partial x_{ij}} =\n",
    "\\sum_{k=1}^{H} v_k \\frac {\\partial \\sigma_{ik}}{\\partial z_{ik}} \\frac {\\partial z_{ik}}{\\partial x_{ij}} = \\sum_{k=1}^{H} v_k w_{jk} \\sigma_{ik}^{(1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the derivatives of $\\sigma$ are given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sigma^{(k)} = \\frac {d^k \\sigma}{dz^k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the values of $\\psi_{ti}$ and $\\vec \\nabla \\psi_{ti}$, we can now compute the values of $G_i$, and then the error function $E$ for the current epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network parameters are the weights and biases of the hidden and output nodes. For an $m$-dinesional input point $\\vec x_i$, and a set of $H$ hidden nodes, we have a total of $N_p = (m+2)H$ parameters in total: a weight for each $x_{ij}$ for each hidden node, a bias for each hidden node, and an output weight for each hidden node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the objective function to minimize is the error $E$, the value of each network parameter $p_{jk}$ (where $p_{jk}$ represents $v_k$, $u_k$, or $w_{jk}$) is updated using a scaled Newton's method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p_{jk,new}=p_{jk} - \\eta \\frac {\\frac {\\partial E}{\\partial {p_{jk}}}} {\\frac {\\partial^2 E}{\\partial {p_{jk}^2}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\eta$ is the *learning rate* (usually $\\alpha < 1$). The learning rate is used to reduce the chance of solution instability due to large values of the correction factor in Newton's method. The derivatives of $E$ are computed using the known form of $G_i$, the network, and the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of parameter updates is repeated until the specified maximum number of training epochs has been applied. Note that the same set of training points is presented during each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the derivatives of $E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first partial derivative of $E$ with respect to any network parameter $p_{jk}$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial E}{\\partial p_{jk}} =\n",
    "\\frac {\\partial}{\\partial p_{jk}} \\sum_{i=1}^{n} G_i^2 =\n",
    "2 \\sum_{i=1}^n G_i \\frac {\\partial G_i}{\\partial p_{jk}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second partial derivative of $E$ with respect to any network parameter $p_{jk}$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial^2 E}{\\partial p_{jk}^2} =\n",
    "\\frac {\\partial}{\\partial p_{jk}} 2 \\sum_{i=1}^n G_i \\frac {\\partial G_i}{\\partial p_{jk}} =\n",
    "2 \\sum_{i=1}^n \\left[G_i \\frac {\\partial^2 G_i}{\\partial p_{jk}^2} + \\left(\\frac {\\partial G_i}{\\partial p_{jk}} \\right)^2 \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general forms of the partial derivatives of $G_i$ are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial G_i}{\\partial p_{jk}} =\n",
    "\\frac {\\partial G_i}{\\partial \\psi_{ti}} \\frac {\\partial \\psi_{ti}}{\\partial p_{jk}} +\n",
    "\\sum_{l=1}^m \\frac {\\partial G_i}{\\partial \\left(\\frac {\\partial \\psi_{ti}}{\\partial x_{il}}\\right)}\n",
    "\\frac {\\partial \\left(\\frac {\\partial \\psi_{ti}}{\\partial x_{il}}\\right)}{\\partial p_{jk}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial^2 G_i}{\\partial p_{jk}^2} =\n",
    "\\frac {\\partial G_i}{\\partial \\psi_{ti}}\n",
    "\\frac {\\partial^2 \\psi_{ti}}{\\partial p_{jk}^2} +\n",
    "\\frac {\\partial^2 G_i}{\\partial \\psi_{ti}^2} \\left(\\frac {\\partial \\psi_{ti}}{\\partial p_{jk}}\\right)^2 +\n",
    "\\sum_{l=1}^m \\left( \\frac {\\partial G_i}{\\partial \\left(\\frac {\\partial \\psi_{ti}}{\\partial x_{il}}\\right)} \\frac {\\partial^2 \\left(\\frac {\\partial \\psi_{ti}}{\\partial x_{il}}\\right)}{\\partial p_{jk}^2} +\n",
    "\\frac {\\partial^2 G_i}{\\partial \\left(\\frac {\\partial \\psi_{ti}}{\\partial x_{ij}}\\right)^2 } \\left(\\frac {\\partial \\frac {\\partial \\psi_{ti}}{\\partial x_{ij}}}{\\partial p_{jk}} \\right)^2 \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives of $N_i$ with respect to the network parameters are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial N_i}{\\partial p_{jk}} =\n",
    "\\frac {\\partial}{\\partial p_{jk}} \\sum_{l=1}^H v_l \\sigma_{il} =\n",
    "\\sum_{l=1}^H \\left( v_l \\frac {\\partial \\sigma_{il}}{\\partial p_{jk}} + \\frac {\\partial v_l}{\\partial p_{jk}} \\sigma_{il} \\right) =\n",
    "\\sum_{l=1}^H \\left( v_l \\sigma_{il}^{(1)} \\frac {\\partial z_{il}}{\\partial p_{jk}} + \\frac {\\partial v_l}{\\partial p_{jk}} \\sigma_{il} \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial^2 N_i}{\\partial p_j^2} =\n",
    "\\sum_{l=1}^H \\left(\n",
    "v_l \\sigma_{il}^{(1)} \\frac {\\partial^2 z_{il}}{\\partial p_{jk}^2} +\n",
    "v_l \\sigma_{il}^{(2)} \\left( \\frac {\\partial z_{il}}{\\partial p_{jk}} \\right)^2 +\n",
    "2 \\frac {\\partial v_l}{\\partial p_{jk}} \\sigma_{il}^{(1)} \\frac {\\partial z_{il}}{\\partial p_{jk}} +\n",
    "\\frac {\\partial^2 v_l}{\\partial p_{jk}^2} \\sigma_{il}\n",
    "\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need the cross-partials of $N_i$ with respect to $x_i$ and the network parameters $p_{jk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial^2 N_i}{\\partial p_{jk} \\partial x_{ij}} =\n",
    "\\frac {\\partial}{\\partial p_{jk}} \\sum_{l=1}^{H} v_l w_{jl} \\sigma_{il}^{(1)} =\n",
    "\\sum_{l=1}^{H} \\left(\n",
    "v_l w_{jl} \\sigma_{il}^{(2)} \\frac {\\partial z_{il}}{\\partial p_{jk}} +\n",
    "v_l \\frac {\\partial w_{jl}}{\\partial p_{jk}} \\sigma_{il}^{(1)} +\n",
    "\\frac {\\partial v_l}{\\partial p_{jk}} w_{jl} \\sigma_{il}^{(1)}\n",
    "\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial^3 N_i}{\\partial p_j^2 \\partial x_i} = \\sum_{l=1}^H \\left [\n",
    "v_l w_{jl} \\sigma_{il}^{(2)} \\frac {\\partial^2 z_{il}}{\\partial p_{jk}^2} +\n",
    "v_l w_{jl} \\sigma_{il}^{(3)} \\left ( \\frac {\\partial z_{il}}{\\partial p_{jk}} \\right )^2 +\n",
    "v_l \\frac {\\partial w_{jl}}{\\partial p_{jk}} \\sigma_{il}^{(2)} \\frac {\\partial z_{il}}{\\partial p_{jk}} +\n",
    "\\frac {\\partial v_l}{\\partial p_{jk}} w_{jl} \\sigma_{il}^{(2)} \\frac {\\partial z_{il}}{\\partial p_{jk}} + \\\\\n",
    "v_l \\frac {\\partial w_{jl}}{\\partial p_{jk}} \\sigma_{il}^{(2)} \\frac {\\partial z_{il}}{\\partial p_{jk}} +\n",
    "v_l \\frac {\\partial^2 w_{jl}}{\\partial p_{jk}^2} \\sigma_{ij}^{(1)} +\n",
    "\\frac {\\partial v_l}{\\partial p_{jk}} \\frac {\\partial w_{jl}}{\\partial p_{jk}} \\sigma_{il}^{(1)} +\n",
    "\\frac {\\partial v_l}{\\partial p_{jk}} w_{jl} \\sigma_{il}^{(2)} \\frac {\\partial z_{il}}{\\partial p_{jk}} +\n",
    "\\frac {\\partial v_l}{\\partial p_{jk}} \\frac {\\partial w_{jl}}{\\partial p_{jk}} \\sigma_{il}^{(1)} +\n",
    "\\frac {\\partial^2 v_l}{\\partial p_{jk}^2} w_{jl} \\sigma_{ij}^{(1)}\n",
    "\\right ] \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these expressions can now be simplified using the following relations between the network parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial v_k}{\\partial v_l} = \\delta_{kl},\n",
    "\\frac {\\partial v_k}{\\partial w_*} = \\frac {\\partial v_k}{\\partial u_*} = 0,\n",
    "\\frac {\\partial^2 v_k}{\\partial p_*^2} = 0\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial w_{jk}}{\\partial w_{il}} = \\delta_{ij} \\delta_{kl},\n",
    "\\frac {\\partial w_{jk}}{\\partial v_*} = \\frac {\\partial w_{jk}}{\\partial u_*} = 0,\n",
    "\\frac {\\partial^2 w_{jk}}{\\partial p_*^2} = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial u_k}{\\partial u_l} = \\delta_{kl},\n",
    "\\frac {\\partial u_k}{\\partial v_*} = \\frac {\\partial u_k}{\\partial w_*} = 0,\n",
    "\\frac {\\partial^2 u_k}{\\partial p_*^2} = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial z_{ik}}{\\partial v_*} = 0,\n",
    "\\frac {\\partial z_{ik}}{\\partial w_{jl}} = x_{ij},\n",
    "\\frac {\\partial z_{ik}}{\\partial u_{jl}} = \\delta_{ij} \\delta_{kl},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial z_{ij}}{\\partial u_j} = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {\\partial z_{ij}}{\\partial w_j} = x_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking through an example problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now walk through a complete problem which will illustrate how to use the nnode1 code to solve a 1st-order ODE IVP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the ODE to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the simple 1st-order ODE, defined on the range $x=[0, 1]$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {dy}{dx} + x y = x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be rearranged into the standard form (1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac {dy}{dx} = f(x,y) = x(1-y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution to this equation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y(x) = 1 + e^{-x^2/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution and its derivative are shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ya(x):\n",
    "    return 1 + exp(-x**2 / 2)\n",
    "\n",
    "# Define the 1st analytical derivative.\n",
    "def dya_dx(x):\n",
    "    return -x * exp(-x**2 / 2)\n",
    "\n",
    "# Define the original differential equation:\n",
    "def F(x, y):\n",
    "    return x * (1 - y)\n",
    "\n",
    "# Define the 1st y-partial derivative of the differential equation.\n",
    "def dF_dy(x, y):\n",
    "    return -x\n",
    "\n",
    "# Define the 2nd y-partial derivative of the differential equation.\n",
    "def d2F_dy2(x, y):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 1\n",
    "n = 100\n",
    "dx = (xmax - xmin) / n\n",
    "#x = np.arange(xmin, xmax, dx) # Give division by zero error\n",
    "x = np.linspace(xmin,xmax,n)\n",
    "y = np.zeros(n)\n",
    "dy_dx = np.zeros(n)\n",
    "for i in range(n):\n",
    "    y[i] = ya(x[i])\n",
    "    dy_dx[i] = dya_dx(x[i])\n",
    "plt.plot(x, y, label = \"$y$\")\n",
    "plt.plot(x, dy_dx, label = \"$dy/dx$\")\n",
    "plt.xlabel(\"x\")\n",
    "#plt.ylabel(\"$d^ky/dx^k$\")\n",
    "plt.legend()\n",
    "plt.title(\"Figure 2: Analytical solution of $dy/dx = x(1-y)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other required definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above also defines several derivative functions that are required for computation of the gradients by the neural network. In addition to the ODE itself (defined as the function F), the first two partial derivatives of the ODE function $f(x,y)$ with respect to $y$ are required (defined as the functions `dF_dy` and `d2F_dy2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary conditions for the ODE must be set before the solution is attempted. For 1st-order ODE BVP, only Dirichlet boundary conditions (BC) are possible. In this case, we will always use the value of $y_t(0)$, denoted as $A$. For this problem, we are interested in a solution over the range $x=[0,1]$. The boundary conditions are therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{equation}\n",
    "x_{min} = 0 \\\\\n",
    "x_{max} = 1 \\\\\n",
    "A = y(0) = 2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this example, an evenly-spaced set of training points will be used to train the neural network. Note that the initial point ($y_t(0)=A$) is not included in the training set, as it would artificially improve the accuracy measures of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nt = 10\n",
    "dx = (xmax - xmin) / nt\n",
    "xt = np.linspace(xmin,xmax,nt)\n",
    "#xt = np.arange(xmin, xmax, dx) + dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that repeated runs of the same ODE will usually result in slightly different solutions, due to the random number generator. To ensure repeatable results, seed the random number generator with a fixed value before each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model to solve the ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network. The call below shows the minimum arguments required to call the nnode1() function. All tunable parameters (learning rate, hidden layer size, number of training epochs) are given default values (0.01, 10, 1000, respectively). The training function returns the computed values of $y$ and $\\frac {dy}{dx}$ at the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nnode1 import nnode1\n",
    "A = ya(xmin)\n",
    "np.random.seed(0)\n",
    "(yt, dyt_dx) = nnode1(xt, F, dF_dy, d2F_dy2, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(xt, yt, label = \"$y$\")\n",
    "plt.plot(xt, dyt_dx, label = \"$dy/dx$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$d^ky/dx^k$\")\n",
    "plt.legend()\n",
    "plt.title(\"Figure 3: Trained solution and derivative\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error in the estimated solution and derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = ya(xt[i])\n",
    "    dy_dx[i] = dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 4: Error in trained solution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try repeating the analysis with a larger number of hidden nodes, and plot the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(yt, dyt_dx) = nnode1(xt, F, dF_dy, d2F_dy2, A, nhid = 20)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = ya(xt[i])\n",
    "    dy_dx[i] = dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 5: Error in trained solution (20 hidden nodes)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try repeating the analysis with a slightly larger learning rate, and plot the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(yt, dyt_dx) = nnode1(xt, F, dF_dy, d2F_dy2, A, eta = 0.02)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = ya(xt[i])\n",
    "    dy_dx[i] = dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 6: Error in trained solution ($\\eta$ = 0.02)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try repeating the analysis with a larger number of training epochs, and plot the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(yt, dyt_dx) = nnode1(xt, F, dF_dy, d2F_dy2, A, maxepochs = 2000)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = ya(xt[i])\n",
    "    dy_dx[i] = dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 7: Error in trained solution (2000 epochs)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a ODE definition module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than entering ODE definitions in this notebook, the required definitions can be entered in a separate Python module, and imported. For example, the previous code is also encapsulated in the module ode00.py, and can be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ode00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the net using the information in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(yt, dyt_dx) = nnode1(xt, ode00.F, ode00.dF_dy, ode00.d2F_dy2, ode00.ymin)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = ode00.ya(xt[i])\n",
    "    dy_dx[i] = ode00.dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 8: Error in trained solution (using ode00.py)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of 1st-order ODEs from the Lagaris paper have been provided in the files lagaris01.py and lagaris02.py. Run them in the same fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "import lagaris01\n",
    "(yt, dyt_dx) = nnode1(xt, lagaris01.F, lagaris01.dF_dy,\n",
    "                      lagaris01.d2F_dy2, lagaris01.ymin, nhid = 40)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = lagaris01.ya(xt[i])\n",
    "    dy_dx[i] = lagaris01.dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 9: Error in trained solution (using lagaris01.py)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "import lagaris02\n",
    "(yt, dyt_dx) = nnode1(xt, lagaris02.F, lagaris02.dF_dy, lagaris02.d2F_dy2,\n",
    "                      lagaris02.ymin, nhid = 40)\n",
    "y = np.zeros(nt)\n",
    "dy_dx = np.zeros(nt)\n",
    "for i in range(nt):\n",
    "    y[i] = lagaris02.ya(xt[i])\n",
    "    dy_dx[i] = lagaris02.dya_dx(xt[i])\n",
    "plt.plot(xt, yt - y, label = \"$y_t-y$\")\n",
    "plt.plot(xt, dyt_dx - dy_dx, label = \"$dy_t/dx - dy/dx$\")\n",
    "plt.xlabel('$x_t$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.title(\"Figure 10: Error in trained solution (using lagaris02.py)\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
